[{"authors":["Eva Zangerle","Christine Bauer","Alain Said"],"categories":null,"content":"","date":1663804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663804800,"objectID":"8ad74b3e2775fc65897ce06dd51d7b2f","permalink":"http://multimethods.github.io/events/perspectives2022_ws/","publishdate":"2022-04-17T00:00:00Z","relpermalink":"/events/perspectives2022_ws/","section":"events","summary":"Workshop on the Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2022) at RecSys 2022.","tags":["workshop","RecSys"],"title":"2nd Workshop: Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2022)","type":"event"},{"authors":["Eva Zangerle","Christine Bauer"],"categories":[],"content":"","date":1660867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661044271,"objectID":"d150707f6006da50e4a289c6af06027b","permalink":"http://multimethods.github.io/publication/zangerle-2022-fevr/","publishdate":"2022-08-19T01:13:45.839142Z","relpermalink":"/publication/zangerle-2022-fevr/","section":"publication","summary":"The comprehensive evaluation of the performance of a recommender system is a complex endeavor: many facets need to be considered in configuring an adequate and effective evaluation setting. Such facets include, for instance, defining the specific goals of the evaluation, choosing an evaluation method, underlying data, and suitable evaluation metrics. In this paper, we consolidate and systematically organize this dispersed knowledge on recommender systems evaluation. We introduce the “Framework for EValuating Recommender systems” (FEVR) that we derive from the discourse on recommender systems evaluation. In FEVR, we categorize the evaluation space of recommender systems evaluation. We postulate that the comprehensive evaluation of a recommender system frequently requires considering multiple facets and perspectives in the evaluation. The FEVR framework provides a structured foundation to adopt adequate evaluation configurations that encompass this required multi-facettedness and provides the basis to advance in the field. We outline and discuss the challenges of a comprehensive evaluation of recommender systems, and provide an outlook on what we need to embrace and do to move forward as a research community.","tags":["evaluation","recommender systems","FEVR","survey"],"title":"Evaluating Recommender Systems: Survey and Framework","type":"publication"},{"authors":["Christine Bauer"],"categories":"special issue","content":"The new journal ACM Transactions on Recommender Systems has just released the first Call for Papers for a special issue: ‘Perspectives on Recommender Systems Evaluation’. The Call for Papers and the submission details are already available. The deadline is November 30, 2022.\n","date":1650816006,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650816006,"objectID":"fbcb72ea4627dde886ae7c3910a4d326","permalink":"http://multimethods.github.io/post/20220424_si_tors/","publishdate":"2022-04-24T18:00:06+02:00","relpermalink":"/post/20220424_si_tors/","section":"post","summary":"Special Issue 'Perspectives on Recommender Systems Evaluation' in ACM Transactions on Recommender Systems.","tags":["special issue","recommender systems","evaluation","CfP"],"title":"SI 'Perspectives on Recommender Systems Evaluation' @ TORS","type":"post"},{"authors":["Christine Bauer"],"categories":["workshop"],"content":"The workshop Perspectives on the Evaluation of Recommender Systems is back at RecSys 2022. The Call for Papers and submission details are already available at the workshop website.\n","date":1650209706,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650209706,"objectID":"46ba367a3405478b3646e9ff9f4ad41c","permalink":"http://multimethods.github.io/post/20220417_perspectives2022/","publishdate":"2022-04-17T17:35:06+02:00","relpermalink":"/post/20220417_perspectives2022/","section":"post","summary":"PERSPECTIVES 2022: The workshop 'Perspectives on the Evaluation of Recommender Systems' is back at RecSys 2022.","tags":["workshop","RecSys"],"title":"Workshop PERSPECTIVES 2022 @ RecSys 2022","type":"post"},{"authors":["Eva Zangerle","Christine Bauer","Alan Said"],"categories":[],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643850671,"objectID":"2d09aebb2545c0acb3c5c693aaee6b8e","permalink":"http://multimethods.github.io/publication/zangerle-2021-sigirforum-perspectives/","publishdate":"2021-12-01T01:13:45.839142Z","relpermalink":"/publication/zangerle-2021-sigirforum-perspectives/","section":"publication","summary":"Evaluation is a central step when it comes to developing, optimizing, and deploying recommender systems. The PERSPECTIVES 2021 workshop at the 15th ACM Conference on Recommender Systems brought together academia and industry to critically reflect on the evaluation of recommender systems. The primary goal of the workshop was to capture the current state of evaluation from different, and maybe even diverging or contradictory perspectives.","tags":[],"title":"Report on the 1st Workshop on the Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2021) at RecSys 2021","type":"publication"},{"authors":["Eva Zangerle","Christine Bauer","Alan Said"],"categories":[],"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634508071,"objectID":"dcd14c039042b1e4eb7ccfda4526e099","permalink":"http://multimethods.github.io/publication/zangerle-2021-perspectives-ceur/","publishdate":"2021-10-18T01:13:45.839142Z","relpermalink":"/publication/zangerle-2021-perspectives-ceur/","section":"publication","summary":"This volume of CEUR-WS proceedings includes papers of the Perspectives on the Evaluation of Recommender Systems Workshop 2021. The workshop is co-located with the ACM Recommender Systems Conference 2021 in Amsterdam, The Netherlands. The goal of the workshop was to capture the current state of evaluation and gauge whether there is—or should be—a different target that recommender systems evaluation should strive for. The workshop primarily addressed the question: “Where should we go from here as a community?” and aimed at coming up with concrete steps for action. We received 17 paper contributions and one abstract submission for the workshop. Each paper contribution received three reviews. We selected 12 papers for publication which will also be presented during the workshop. Furthermore, we accepted one abstract submission to be presented at the workshop. We would like to thank the members of the program committee for their valuable reviews and suggestions. We also thank the authors for their submissions and contributions to the workshop.","tags":[],"title":"Proceedings of the Perspectives on the Evaluation of Recommender Systems Workshop 2021","type":"publication"},{"authors":["Eva Zangerle","Christine Bauer","Alan Said"],"categories":[],"content":"","date":1632528e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632273071,"objectID":"ab79d68f3673ab6d8c4e0e4841f8fedc","permalink":"http://multimethods.github.io/publication/zangerle-2021-perspectives/","publishdate":"2021-08-08T01:13:45.839142Z","relpermalink":"/publication/zangerle-2021-perspectives/","section":"publication","summary":"Evaluation is a cornerstone in the process of developing and deploying recommender systems. The PERSPECTIVES workshop brought together academia and industry to critically reflect on the evaluation of recommender systems. Particularly, the workshop aimed to shed light on the different, and maybe even diverging or contradictory perspectives on the evaluation of recommender systems. Papers reporting a reflection on problems regarding recommender systems evaluation and lessons learned were solicited. The workshop combined flash presentations of accepted papers, a keynote from industry, and an interactive part with discussions in break-out rooms as well as in the plenum. The workshop complemented the program of the main conference as it emphasized problems and lessons learned, fostered exchange integrating various perspectives on evaluation, and sought to move the recommender systems community forward as an outcome of the workshop.","tags":[],"title":"Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES)","type":"publication"},{"authors":["Eva Zangerle","Christine Bauer","Alain Said"],"categories":null,"content":"               Eva Zangerle, Christine Bauer, Alan Said  (2021). Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES). 15th ACM Conference on Recommender System.  PDF  Cite  DOI   ","date":1632528e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632528e3,"objectID":"cb3216d3622f2e5f3102fc09080d9a44","permalink":"http://multimethods.github.io/events/perspectives2021_ws/","publishdate":"2021-09-25T00:00:00Z","relpermalink":"/events/perspectives2021_ws/","section":"events","summary":"Workshop on the Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2021) at RecSys 2021.","tags":["workshop","RecSys"],"title":"Workshop on the Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2021)","type":"event"},{"authors":["Christine Bauer"],"categories":null,"content":"","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"60f80d55b749f9cf9e5d94b96f393995","permalink":"http://multimethods.github.io/events/tu01_umap2021_multimethods/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/events/tu01_umap2021_multimethods/","section":"events","summary":"Tutorial on Multi-Method Evaluation of Adaptive Systems at UMAP 2021.","tags":["tutorial","UMAP","multi-methods","evaluation","adaptive systems","recsys"],"title":"Multi-Method Evaluation of Adaptive Systems","type":"event"},{"authors":["Christine Bauer"],"categories":[],"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628385072,"objectID":"540f5bc38f45bfef79a6623d344d7ac2","permalink":"http://multimethods.github.io/publication/bauer-2021-multimethods-tutorial/","publishdate":"2021-08-08T01:13:46.692755Z","relpermalink":"/publication/bauer-2021-multimethods-tutorial/","section":"publication","summary":"When evaluating personalized or adaptive systems, we frequently rely on one single evaluation objective and one single method. This remains us with “blind spots”. A comprehensive evaluation may require a thoughtful integration of multiple methods. This tutorial (i) demonstrates the wide variety of dimensions to be eval- uated, (ii) outlines the methodological approaches to evaluate these dimensions, (iii) pinpoints the blind spots when using only one ap- proach, (iv) demonstrates the benefits of multi-method evaluation, and (v) outlines the basic options how multiple methods can be integrated into one evaluation design. Participants familiarize with the wide spectrum of opportunities how adaptive or personalized systems may be evaluated, and have the opportunity to come up with evaluation designs that comply with the four basic options of multi-method evaluation. The ultimate learning objective is to stimulate the critical reflection of one’s own evaluation practices and those of the community at large.","tags":[],"title":"Multi-Method Evaluation of Adaptive Systems","type":"publication"},{"authors":["Christine Bauer"],"categories":["workshop"],"content":"We am happy to announce that we – Eva Zangerle, Christine Bauer, and Alan Said – will co-organize the workshop Perspectives on the Evaluation of Recommender Systems at RecSys 2021.\nFor more infos, please visit the workshop website.\n","date":1616112e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616112e3,"objectID":"fe0b4e0d4080a651de96d5385216009a","permalink":"http://multimethods.github.io/post/20210319_perspectives2021/","publishdate":"2021-03-19T00:00:00Z","relpermalink":"/post/20210319_perspectives2021/","section":"post","summary":"We am happy to announce that we – Eva Zangerle, Christine Bauer, and Alan Said – will co-organize the workshop Perspectives on the Evaluation of Recommender Systems at RecSys 2021.","tags":["workshop","RecSys"],"title":"Workshop PERSPECTIVES 2021 @ RecSys 2021","type":"post"},{"authors":["Christine Bauer"],"categories":["tutorials"],"content":"Christine Bauer will hold a tutorial on Multi-Method Evaluation of Adaptive Systems at UMAP 2021. UMAP – including the tutorial – will be held fully online.\n","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"67776c21bce685e812c27f92db46d20b","permalink":"http://multimethods.github.io/post/20210317_umap2021_tutorial/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/post/20210317_umap2021_tutorial/","section":"post","summary":"Christine Bauer will hold a tutorial on Multi-Method Evaluation of Adaptive Systems at UMAP 2021. UMAP – including the tutorial – will be held fully online.","tags":["tutorial","UMAP"],"title":"Tutorial on Multi-Methods Evaluation at UMAP2021","type":"post"},{"authors":["Christine Bauer"],"categories":["tutorials"],"content":"While CHIIR 2020 had unfortunately to be cancelled due to current global situation with Covid-19, the paper accompanying the tutorial on multi-method evaluation (that was supposed to be held at CHIIR 2020) is published:\n Christine Bauer  (2020). Multi-Method Evaluation: Leveraging Multiple Methods to Answer What You Were Looking For. 2020 Conference on Human Information Interaction and Retrieval.  PDF  Cite  DOI   ","date":1584144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584144e3,"objectID":"514caf97c5fe9f5443d20ff623ef4c7c","permalink":"http://multimethods.github.io/post/20200314_chiir2020_tutorial_paper/","publishdate":"2020-03-14T00:00:00Z","relpermalink":"/post/20200314_chiir2020_tutorial_paper/","section":"post","summary":"While CHIIR 2020 had unfortunately to be cancelled due to current global situation with Covid-19, the paper accompanying the tutorial on multi-method evaluation (that was supposed to be held at CHIIR 2020) is published:","tags":["tutorial","CHIIR"],"title":"Details on CHIIR 2020 tutorial on multi-method evaluation","type":"post"},{"authors":["Christine Bauer"],"categories":[],"content":"","date":1584144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628385073,"objectID":"3de209f542ea383a1917a94fde738103","permalink":"http://multimethods.github.io/publication/bauer-2020-chiir-tutorial-multimethods/","publishdate":"2021-08-08T01:13:47.574778Z","relpermalink":"/publication/bauer-2020-chiir-tutorial-multimethods/","section":"publication","summary":"Research in the field of information retrieval and recommendation mostly focuses on one single evaluation method and one single quality objective. On the one hand, many research endeavors focus on system-centric evaluation from an algorithmic perspective and consider the context of use only to a minor extent. On the other hand, there are research endeavors focusing on user-centric approaches to the design and evaluation of systems. However, algorithmic quality and perceived quality of user experience do not necessarily match. Thus, it is essential for system evaluation to substantially integrate multiple evaluation methods that cover a variety of relevant aspects and perspectives. Only such an integrated combination of methods may lead to a deep understanding of users, their behavior, and experience in their interaction with a system. This half-day tutorial follows the objective to raise awareness in the CHIIR community concerning the significance of using multiple methods in the evaluation of information retrieval and recommender systems. The tutorial illustrates the ''blind spots'' when using single methods. It introduces the concept of ''multi-method evaluation'' and discusses its benefits and challenges. While multi-method evaluations may be designed very flexibly, the tutorial presents broadly-defined basic options of how multiple methods may be integrated in an evaluation design. In group work, participants are encouraged to select and fine-tune a specific design that best matches their research endeavor's purpose.","tags":[],"title":"Multi-Method Evaluation: Leveraging Multiple Methods to Answer What You Were Looking For","type":"publication"},{"authors":["Christine Bauer"],"categories":["tutorials"],"content":"Unfortunately, CHIIR 2020 was cancelled. So, with it, also the tutorial on multi-method evaluation is cancelled.\nWe are seeking out for new opportunities for tutorials. We’ll keep you posted on this website.\nYes, it\u0026#39;s sad, but true. CHIIR 2020 has been cancelled due to the COVID-19 health crisis. We will miss seeing you all in Vancouver. Stay healthy and CHIIR-ful, everyone! #CHIIR2020\n— ACM CHIIR 2023 (@ACM_CHIIR) March 6, 2020  ","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"daf93fc5ffd8c3e6ff37e7fbea48905a","permalink":"http://multimethods.github.io/post/20200306_chiir2020_cancelled/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/20200306_chiir2020_cancelled/","section":"post","summary":"Unfortunately, CHIIR 2020 was cancelled. So, with it, also the tutorial on multi-method evaluation is cancelled.\nWe are seeking out for new opportunities for tutorials. We’ll keep you posted on this website.","tags":["tutorial","CHIIR"],"title":"CHIIR Cancelled","type":"post"},{"authors":["Christine Bauer","Eva Zangerle"],"categories":[],"content":"","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628385073,"objectID":"5c18090a6733ab3ab23964e8fe63050c","permalink":"http://multimethods.github.io/publication/bauer-2019-multimethod-multistakeholder/","publishdate":"2021-08-08T01:13:47.714286Z","relpermalink":"/publication/bauer-2019-multimethod-multistakeholder/","section":"publication","summary":"In this paper, we focus on recommendation settings with multiple stakeholders with possibly varying goals and interests, and argue that a single evaluation method or measure is not able to evaluate all relevant aspects in such a complex setting. We reason that employing a multi-method evaluation, where multiple evaluation methods or measures are combined and integrated, allows for get- ting a richer picture and prevents blind spots in the evaluation outcome.","tags":[],"title":"Leveraging Multi-Method Evaluation for Multi-Stakeholder Settings","type":"publication"},{"authors":["Ilknur Celik","Ilaria Torre","Frosina Koceva","Christine Bauer","Eva Zangerle","Bart Knijnenburg"],"categories":[],"content":"","date":1531008e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628385074,"objectID":"e3be25a8ca7838b77e31f28b266ca079","permalink":"http://multimethods.github.io/publication/celik-2018-multimethods-umap-workshop/","publishdate":"2021-08-08T01:13:49.106428Z","relpermalink":"/publication/celik-2018-multimethods-umap-workshop/","section":"publication","summary":"","tags":[],"title":"UMAP 2018 Intelligent User-Adapted Interfaces: Design and Multi-Modal Evaluation (IUadaptMe) Workshop Chairs' Welcome \u0026 Organization","type":"publication"},{"authors":["Ilknur Celik","Ilaria Torre","Frosina Koceva","Christine Bauer","Eva Zangerle","Bart P. Knijnenburg"],"categories":null,"content":"       Ilknur Celik, Ilaria Torre, Frosina Koceva, Christine Bauer, Eva Zangerle, Bart Knijnenburg  (2018). UMAP 2018 Intelligent User-Adapted Interfaces: Design and Multi-Modal Evaluation (IUadaptMe) Workshop Chairs\u0026#39; Welcome \u0026amp; Organization. 26th Conference on User Modeling, Adaptation and Personalization.  PDF  Cite  DOI   ","date":1531008e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008e3,"objectID":"5300bca52310d18944373d9bf018abdb","permalink":"http://multimethods.github.io/events/iuadaptme2018/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/events/iuadaptme2018/","section":"events","summary":"Workshop on Intelligent User-Adapted Interfaces: Design and Multi-Modal Evaluation (IUadaptMe 2018) at UMAP 2018.","tags":["workshop","UMAP","evaluation"],"title":"UMAP 2018 Workshop: Intelligent User-Adapted Interfaces: Design and Multi-Modal Evaluation (IUadaptMe 2018)","type":"event"},{"authors":["Christine Bauer","Eva Zangerle"],"categories":[],"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628385074,"objectID":"5d3400a6426220da1581df3a59864785","permalink":"http://multimethods.github.io/publication/bauer-2018-imbalance/","publishdate":"2021-08-08T01:13:49.245282Z","relpermalink":"/publication/bauer-2018-imbalance/","section":"publication","summary":"In the field of music recommender systems, country-specific aspects have received little attention, although it is known that music perception and preferences are shaped by culture; and culture varies across countries. Based on the LFM-1b dataset (including 53,258 users from 47 countries), we show that there are significant country-specific differences in listeners' music consumption behavior with respect to the most popular artists listened to. Results indicate that, for instance, Finnish users' listening behavior is farther away from the global mainstream, while United States' listeners are close to the global mainstream. Relying on rating prediction experiments, we tailor recommendations to a user's level of preference for mainstream (defined on a global level and on a country level) and the user's country. Results suggest that, in terms of rating prediction accuracy, a combination of these two filtering strategies works particularly well for users of countries far away from the global mainstream.","tags":[],"title":"Information imbalance and responsibility in recommender systems","type":"publication"}]